{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Exercise 1: Linear Regression\n",
    "This notebook is a Python implementation of Exercise 1 from the Machine Learning Course given by Andrew Ng on Coursera. It follows the live script including the optional excercises and includes a interactive plot for the cost function. At the end of the notebook there is also a simple implementation with scikit-learn added.<br>\n",
    "**Author:** Pascal Wenger<br>\n",
    "**Created:** 07.Sept 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, interactive\n",
    "import seaborn as sns\n",
    "\n",
    "print('#'*79) # guidance python pep: maximum lenght 79 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (the ex1data1.txt must be in the same directoy as this notebook)\n",
    "path = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(path, \"ex1data1.txt\"), header = None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rawdata\n",
    "sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(22/2.54, 10/2.54))\n",
    "ax.plot(df.iloc[:,0], df.iloc[:,1], 'x', color='red', mew=1)\n",
    "ax.set_title(\"training data: profit vs population\")\n",
    "ax.set_xlabel(\"population of city in 10'000s\")\n",
    "ax.set_ylabel(\"profit in $10'000s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter settings for Gradient Descent:\n",
    "\n",
    "# number of training data:\n",
    "m = len(df.iloc[:,1])\n",
    "\n",
    "# X = population, y = profit\n",
    "X = df.iloc[:,0].values\n",
    "y = df.iloc[:,1].values\n",
    "  \n",
    "# Add column of ones to X (for intercept term):\n",
    "Xtrain = np.column_stack((np.ones((m,1)), X))\n",
    "ytrain = y.reshape((m,1))\n",
    "\n",
    "# initialize fitting parameters\n",
    "theta_0 = np.zeros((2, 1))\n",
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost function:\n",
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    theta: row vector with parameters\n",
    "    X    : matrix with input features \n",
    "           rows: number of training examples\n",
    "           cols: number of features\n",
    "    y    : row vector with targets of training examples\n",
    "    \"\"\"\n",
    "    # number of training examples:\n",
    "    m = len(y)\n",
    "    \n",
    "    # cost function J:\n",
    "    J = 1/(2*m) * np.sum((X @ theta - y)**2)\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display initial cost\n",
    "J = compute_cost(Xtrain, ytrain, theta_0)\n",
    "print(\"Initial value for cost function J \" +\n",
    "      \"with theta =[0,0]: {:0.4f}.\".format(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define gradient descent function:\n",
    "def gradient_descent(X, y, theta_ini, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    X        : matrix with input features \n",
    "               rows: number of training examples\n",
    "               cols: number of features\n",
    "    y        : row vector with targets of training examples   \n",
    "    theta_ini: row vector with initial parameters\n",
    "    alpha    : learning rate, scalar value\n",
    "    num_iters: number of iterations, scalar value\n",
    "    \"\"\"\n",
    "    # initialize some useful values:\n",
    "    m = len(y); # number of training examples\n",
    "    J_history = np.zeros((num_iters+1, 1))\n",
    "    # initial cost value:\n",
    "    J_history[0] = compute_cost(X, y, theta_ini)\n",
    "    \n",
    "    # deep copys of theta_ini (to avoid any interferences):\n",
    "    theta = np.copy(theta_ini)\n",
    "    theta_new = np.copy(theta_ini)\n",
    "    \n",
    "    # compute gradient descent and its cost functions:\n",
    "    for iter in range(num_iters):\n",
    "        \n",
    "        for j in range(len(theta_ini)):\n",
    "            # @ is short version for np.dot()\n",
    "            dJ = np.sum(((X @ theta)[:,0] - y[:,0]) * X[:,j])\n",
    "            \n",
    "            theta_new[j] = theta[j] - alpha/m * dJ\n",
    "            \n",
    "        theta[:] = theta_new[:]\n",
    "        J_history[iter+1] = compute_cost(X, y, theta)\n",
    "    \n",
    "    return J_history, theta            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent\n",
    "J_history, theta  = gradient_descent(Xtrain, ytrain, theta_0, \n",
    "                                     alpha, iterations) \n",
    "\n",
    "# print final theta:\n",
    "print(\"Theta computed from gradient descent: \"+\n",
    "      \"{:0.6f}, {:0.6f}.\".format(theta[0,0], theta[1,0]))\n",
    "print(\"Minimum of cost function J: {:.4f}\".format(J_history[-1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot linear fit:\n",
    "fig, ax = plt.subplots(figsize=(22/2.54, 10/2.54))\n",
    "\n",
    "ax.plot(df.iloc[:,0], df.iloc[:,1], 'x', color='red', mew=1,\n",
    "        label = \"training data\")\n",
    "\n",
    "ax.plot(df.iloc[:,0], theta[0,0] + theta[1,0]*df.iloc[:,0], '-',\n",
    "        color='blue', label = \"linear regression\")\n",
    "\n",
    "ax.set_xlabel(\"population of city in 10'000s\")\n",
    "ax.set_ylabel(\"profit in $10'000s\")\n",
    "plt.legend(frameon=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for population sizes of 35,000 and 70,000:\n",
    "print(\"For population = 35'000, we predict a profit of \",\n",
    "      \"${:0.6f}\".format((theta[0,0] + theta[1,0]*3.5)*1E4))\n",
    "\n",
    "print(\"For population = 70'000, we predict a profit of \",\n",
    "      \"${:0.6f}\".format((theta[0,0] + theta[1,0]*7.0)*1E4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing J(theta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "X, Y = np.meshgrid(theta0_vals, theta1_vals)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((len(theta0_vals), len(theta1_vals)))\n",
    "\n",
    "# fill out J_vals\n",
    "for i in range(len(theta0_vals)):\n",
    "    for j in range(len(theta1_vals)):\n",
    "        t = np.array([ [theta0_vals[i]], [theta1_vals[j]] ])   \n",
    "        J_vals[i,j] = compute_cost(Xtrain, ytrain, t)\n",
    "\n",
    "# plot data:\n",
    "sns.set(font_scale=1.0)\n",
    "fig = plt.figure(figsize = (20/2.54, 25/2.54))\n",
    "\n",
    "# surface plot:\n",
    "ax = fig.add_subplot(2, 1, 1, projection='3d')\n",
    "ax.plot_surface(X, Y, J_vals.T, antialiased=True, cmap=plt.cm.jet)\n",
    "ax.set_xlabel(r'$\\theta_\\mathrm{0}$')\n",
    "ax.set_ylabel(r'$\\theta_\\mathrm{1}$')\n",
    "ax.set_zlabel(r'J$(\\theta)$')\n",
    "ax.set_xlim([-10,10])\n",
    "ax.set_ylim([-1, 4])\n",
    "ax.view_init(20, 225)\n",
    "\n",
    "# contour plot:\n",
    "sns.set(font_scale=1.2)\n",
    "ax = fig.add_subplot(2, 1, 2)\n",
    "ax.contour(X, Y, J_vals.T, np.logspace(-2, 3, num=20), cmap=plt.cm.jet)\n",
    "ax.plot(theta[0], theta[1], 'rx', markerSize=15, markeredgewidth=3)\n",
    "ax.set_xlabel(r'$\\theta_\\mathrm{0}$')\n",
    "ax.set_ylabel(r'$\\theta_\\mathrm{1}$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (the ex1data2.txt must be in the same directoy as this notebook)\n",
    "path = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(path, \"ex1data2.txt\"), header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(df)\n",
    "X = df.iloc[:,0:2].values\n",
    "y = df.iloc[:,2].values\n",
    "\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))\n",
    "print(\"Number of training examples: {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature normalization function:\n",
    "def feature_normalization(X):\n",
    "    \"\"\"\n",
    "    normalize feature matrix \n",
    "    mu :      mean value per feature\n",
    "    sig:      standard devitation per feature\n",
    "    equation: (x - mu) / std\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sig = np.std(X, axis=0)\n",
    "    \n",
    "    X_norm = (X - mu) / sig\n",
    "    \n",
    "    return sig, mu, X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize feature matrix:\n",
    "sig, mu, X = feature_normalization(X)\n",
    "\n",
    "# add x0=1 column to feature matrix (for intercept term):\n",
    "Xtrain = np.column_stack((np.ones((m,1)), X))\n",
    "ytrain = y.reshape((m,1))\n",
    "\n",
    "print(\"Shape of Xtrain: {}\".format(Xtrain.shape))\n",
    "print(\"Shape of ytrain: {}\".format(ytrain.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent\n",
    "# set parameters:\n",
    "alpha = 0.1\n",
    "iterations = 400\n",
    "theta_0 = np.zeros((Xtrain.shape[1],1))\n",
    "\n",
    "# run gradient descent:\n",
    "J_history, theta_gd = gradient_descent(Xtrain, ytrain, theta_0, \n",
    "                                       alpha, iterations) \n",
    "\n",
    "# print final theta:\n",
    "print(\"Theta computed from Gradient Descent: \\n\"+\n",
    "      \"{:0.3f}, {:0.3f}, {:0.3f}.\".format(theta_gd[0,0], theta_gd[1,0], \n",
    "                                          theta_gd[2,0]))\n",
    "print(\"Theta shape: {}\".format(theta_gd.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate price of a 1650sq-ft, 3 bedroom house:\n",
    "feature = np.array([1650, 3]).reshape((1,2))\n",
    "\n",
    "# feature normalize:\n",
    "feature_norm = (feature - mu) / sig\n",
    "\n",
    "# add x0=1 column to feature matrix:\n",
    "feature_norm = np.column_stack((np.ones((1)), feature_norm))\n",
    "\n",
    "price = feature_norm @ theta_gd\n",
    "\n",
    "print(\"Estimated price for a 1650sq-ft,\",\n",
    "      \"3-bedroom flat: ${:0.0f}.\".format(price[0,0]))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_in = Xtrain.copy()\n",
    "y_in = ytrain.copy()\n",
    "\n",
    "# define function for intreractive plot\n",
    "def cost_parameters(alpha = 0.1, iterations = 400):\n",
    "    # set figure:\n",
    "    sns.set(font_scale=1.2)\n",
    "    fig, ax = plt.subplots(figsize=(22/2.54, 10/2.54))\n",
    "    \n",
    "    # Cost function to vary::\n",
    "    def J_iter(X_in=X_in, y_in=y_in, \n",
    "               alpha=alpha, n_iter=iterations):\n",
    "        # set initial theta to zeros:\n",
    "        theta_0 = np.zeros((Xtrain.shape[1],1))\n",
    "        # compute cost function for paramters\n",
    "        J_hist, _= gradient_descent(X_in, y_in, theta_0, alpha, n_iter)\n",
    "        return J_hist\n",
    "\n",
    "    # visualizing J(theta):\n",
    "    ax.plot(J_iter(), color='red', linewidth=5)\n",
    "    s0 = \"       \"\n",
    "    s1 = \"cost function vs iteration for: \"\n",
    "    s2 = \"$\\\\alpha$ = {:4.3f}  \".format(alpha)\n",
    "    s3 = \"and ${\\\\theta_{\\mathrm{ini}}}$= [0;0;0]\"\n",
    "    ax.set_title(s0 + s1 + s2 + s3)\n",
    "    ax.set_xlabel(\"number of iterations\")\n",
    "    ax.set_ylabel(\"cost function J(theta)\")\n",
    "    ax.set_xlim([-25,225])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display interactive plot:\n",
    "w = interactive(cost_parameters, alpha=(0.01, 1.5), iterations=(50, 200))\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with multiple variables using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define normal equation:\n",
    "def normal_equation(X, y):\n",
    "    \"\"\"\n",
    "    X        : matrix with input features \n",
    "               rows: number of training examples\n",
    "               cols: number of features\n",
    "    y        : row vector with targets of training examples   \n",
    "    theta    : row vector with parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "  \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (the ex1data2.txt must be in the same directoy as this notebook)\n",
    "path = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(path, \"ex1data2.txt\"), header = None)\n",
    "\n",
    "m = len(df)\n",
    "X = df.iloc[:,0:2].values\n",
    "y = df.iloc[:,2].values\n",
    "\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))\n",
    "print(\"Number of training examples: {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x0=1 column to feature matrix:\n",
    "Xtrain = np.column_stack((np.ones((m,1)), X))\n",
    "ytrain = y.reshape((m,1))\n",
    "print(\"Shape of Xtrain: {}\".format(Xtrain.shape))\n",
    "print(\"Shape of ytrain: {}\".format(ytrain.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run normal equation\n",
    "theta_n = normal_equation(Xtrain, ytrain)\n",
    "\n",
    "# print final theta from normal equation:\n",
    "print(\"Theta computed from normal equation: \"+\n",
    "      \"\\n{:0.6f}, {:0.6f}, {:0.6f}\".format(theta_n[0,0], theta_n[1,0],\n",
    "                                          theta_n[2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate price of a 1650sq-ft, 3 bedroom house:\n",
    "feature = np.array([1650, 3]).reshape((1,2))\n",
    "\n",
    "# add x0=1 column to feature matrix(for intercept term):\n",
    "feature = np.column_stack((np.ones((1)), feature))\n",
    "\n",
    "price = feature @ theta_n\n",
    "\n",
    "print(\"Estimated price for a 1650sq-ft,\",\n",
    "      \"3-bedroom flat: ${}.\".format(int(price)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with multiple variables using sckit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data (the ex1data2.txt must be in the same directoy as this notebook)\n",
    "path = os.getcwd()\n",
    "df = pd.read_csv(os.path.join(path, \"ex1data2.txt\"), header = None)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:2]\n",
    "y = df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "prediction = reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate price of a 1650sq-ft, 3 bedroom house:\n",
    "feature = np.array([1650, 3]).reshape((1,2))\n",
    "\n",
    "# add x0=1 column to feature matrix(for intercept term):\n",
    "feature = np.column_stack((np.ones((1)), feature))\n",
    "\n",
    "price = feature @ np.array([[reg.intercept_],[reg.coef_[0]],[reg.coef_[1]]])\n",
    "print(\"Estimated price for a 1650sq-ft,\",\n",
    "      \"3-bedroom flat: ${}.\".format(int(price)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Theta computed wiht Linear Regression from sclit-learn: \\n\" +\n",
    "      \"[{:.6f}, {:.6f}, {:.6f}]\".format(reg.intercept_, reg.coef_[0],\n",
    "                                        reg.coef_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with Theta given by the normale equation:\n",
    "print(\"Theta computed from Normal Equation: \"+\n",
    "      \"\\n{:0.6f}, {:0.6f}, {:0.6f}\".format(theta_n[0,0], theta_n[1,0],\n",
    "                                          theta_n[2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# END of File #####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
